\section{Options}

Despite not being the earliest in the timeline, one of the simplest and sort-after frameworks in the HRL community is that of options. The basic idea of options is to take the solutions to each of the subproblems and encapsulate them as a single action. With the extension of this brainchild, an option (temporally-extended actions) is analogous to the macro actions in the planning world. Option is an encapsulated policy, which consists of a stationary stochastic policy \(\pi:S \times U_{s \in \mathcal{S}}  \mathcal{A}_s \rightarrow [0,1]\) , an initiation set \(\mathcal{I} \subseteq  \mathcal{S}\) which includes of all the states where the option holds true, and a termination condition \(\beta:\mathcal{S} \rightarrow [0,1]\) which gives the probability of a state being the terminal one for the option \cite{Sutton}. The option \( \langle \mathcal{I},\pi,\beta \rangle \) is available in state \(s\) only if \(s \in \mathcal{I} \). If the option is executed, then the actions are selected according to \(\pi\) until termination, determined by \(\beta\). Let the current state within the option be \(s\). Then the option chooses the next action determined by the probability \(\pi(s,a)\). This execution of action takes the option to the state \(s'\), where the option terminates with probability \(\beta(s')\). Upon termination, the agent can choose another option. 

\iffalse
Consider a humanoid robot which has the task of opening the door to exit a room. This problem can further have subproblems like reaching, grasping and turning the knob. The initiation set considers of all the states where the agent is inside the room. Actions include navigating within the room, grasping the door knob and turning it to leave the room. The policies determine the ways to navigate through this small world. On exiting the room, \(\beta(s)=1\), which is the only terminal state for this example. This option can further be a part of any larger problem. In other words, options can act as actions within another option. These chain of options gives the hierarchy. Thus, in HRL a set of options replaces the set of actions. \\
\fi

Options are of two types: Markov and Semi-Markov. As it is evident, in a Markov option, \(\pi\) is defined only on the current state. This implies that irrespective of which state the option is, the same action is executed. A Semi-Markov Option keeps track of history since the initiation of option, and NOT the history before the initiation. Consider a grid world, with an option where agent aims to take two steps right and then one down. The state is given by the current locational coordinates of the agent in the grid world. The action performed by the agent in a state depends on his start state. For instance, if the agent is in \((0,0)\), then he will move right to \((1,0)\), then right to \((2,0)\) and then down to \((2,1)\). However, if the agent starts at \((1,0)\), then the agent will move right at \((2,0)\) to \((3,0)\) instead of proceeding down. Thus, the action to be taken at \((2,0)\) depends on the starting state of the option. Hence, this option is Semi-Markov. However, if the options task was just to move right till the wall is hit. In this case, the options action for a state is dependent only on the current state and hence, it is Markov. Note that even if an option encapsulates a Markov option, then the encapsulating option is Semi-Markov. 

Options, act on top of MDP, which is discrete-timed. Options on top of MDP, gives rise to SMDP. Due to this SMDP nature, SMDP Q-learning update can be used to update the policies over the options. While learning, it is assumed that the internal policies of an option are complete and frozen. In other words, the internal policies of an options are already provided. The learning for options, thus, occurs only at the terminal states as the decision for the next option is taken at this state. One issue before applying learning is that the environment can contain both basic actions and options. This problem is solved by this framework by consider actions as options with just one state in the initiation set, \(\mathcal{I}=s\) and the termination condition for that state set to 1, \(\beta(s)=1\). 

Let \(\mathcal{O}_s\) be the set of admissible options possible for state \(s \in \mathcal{S}\). The policies over the options are given by \(\mu:\mathcal{S} \times U_{s \in \mathcal{S}} O_s \rightarrow [0,1]\). The policy \(\mu\) selects option \(o\) in state \(\mathcal{S}\) with probability \(\mu(s,o)\). \(R(s,o)\) denotes the expected reward for executing option \(o\) in state \(s\). \(P(s' |s,o)\) is the transition probability of \(o\) terminating in state \(s'\) on starting in s after \(\tau\) time steps. \(V_o\) and \(Q_o\) are the value functions for following the policy over the options in \(\mathcal{O}\) for the states in \(S\). The Bellman equations for the value function and action-value function is given by: 

\begin{equation}
    V_\mathcal{O}^* (s)=\max_{o \in  \mathcal{O}_s }⁡[R(s,o)  +\sum_{s'}P(s' \mid s,o) V_\mathcal{O}^* (s')]
\end{equation}

\begin{equation}
    Q_\mathcal{O}^* (s,o) = R(s,o) + \sum_{s'} P(s'\mid s,o)   \max_{o' \in \mathcal{O}_s } ⁡Q_\mathcal{O}^* (s',o')]
\end{equation}

The SMDP Q-learning update for this action-value function is:

\begin{equation}
    Q_{k+1} (s,o)=(1-\alpha_k ) Q_k (s,o)+\alpha_k [r+ \gamma^\tau.   \max_{o' \in O_{s'}}⁡ Q_k (s',o' )]
\end{equation}


This update is applied upon the termination of \(o\) at state \(s'\) after executing for \(\tau\) time-steps, and \(r\) is the return accumulated during \(o^'\) s execution.  \\

    

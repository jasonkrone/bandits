\section{Options}

Despite not being the earliest in the timeline, one of the simplest frameworks in the HRL community is options  \cite{Sutton}. The basic idea of options is to take the solutions to each of the subproblems and encapsulate them as a single action. Using this extension, an option (or temporally-extended actions) is analogous to macro actions in the planning world. Option are encapsulated policies, consisting of a stationary stochastic policy \(\pi:S \times U_{s \in \mathcal{S}}  \mathcal{A}_s \rightarrow [0,1]\) , an initiation set \(\mathcal{I} \subseteq  \mathcal{S}\) which includes of all the states where the option holds true, and a termination condition \(\beta:\mathcal{S} \rightarrow [0,1]\) which gives the probability of a state being terminal. The option \( \langle \mathcal{I},\pi,\beta \rangle \) is available in state \(s\) only if \(s \in \mathcal{I} \). If the option is executed, then the actions are selected according to \(\pi\) until termination, determined by \(\beta\). Given an option in state \(s\) the next action is determined by the probability \(\pi(s,a)\). This execution of action takes the option to the state \(s'\) where the option terminates with probability \(\beta(s')\). Upon termination, the agent can choose another option.

\iffalse
Consider a humanoid robot which has the task of opening the door to exit a room. This problem can further have subproblems like reaching, grasping and turning the knob. The initiation set considers of all the states where the agent is inside the room. Actions include navigating within the room, grasping the door knob and turning it to leave the room. The policies determine the ways to navigate through this small world. On exiting the room, \(\beta(s)=1\), which is the only terminal state for this example. This option can further be a part of any larger problem. In other words, options can act as actions within another option. These chain of options gives the hierarchy. Thus, in HRL a set of options replaces the set of actions. \\
\fi

There are two types of options: Markov and Semi-Markov. In a Markov option \(\pi\) is defined only on the current state, implying that the same action is executed irrespective of the current state. A Semi-Markov Option keeps track of history since the initiation, but not history before the initiation. Consider a grid world, with an option where the agent aims to take two steps right and then one down. The state is given by the current locational coordinates of the agent in the grid world. The action performed by the agent in a state depends on his start state. For instance, if the agent is in \((0,0)\), then he will move right to \((1,0)\), then right to \((2,0)\) and then down to \((2,1)\). However, if the agent starts at \((1,0)\), then the agent will move right at \((2,0)\) to \((3,0)\) instead of proceeding down. Thus, the action taken at \((2,0)\) depends on the starting state of the option making the option Semi-Markov. However, if the options task was just to move right util the wall is hit the options action for a state depends only on the current state and is Markov. Note that even if an option encapsulates a Markov option the encapsulating option is Semi-Markov.

Options acting on top of discrete time MDPs give rise to SMDPs. By nature of SMDPs Q-learning updates can be used to update the policies over the options. While learning it is assumed that the internal policies of an option are complete and frozen. In other words, the internal policies of an options are already provided. The learning for options, thus, occurs only at the terminal states as the decision for the next option is taken at this state. One issue before applying learning is that the environment can contain both basic actions and options. Options solve this by considering actions as options with just one state in the initiation set, \(\mathcal{I}=s\) and the termination condition for that state set to 1, \(\beta(s)=1\).

% Let \(\mathcal{O}_s\) be the set of admissible options possible for state \(s \in \mathcal{S}\). The policies over the options are given by \(\mu:\mathcal{S} \times U_{s \in \mathcal{S}} O_s \rightarrow [0,1]\). The policy \(\mu\) selects option \(o\) in state \(\mathcal{S}\) with probability \(\mu(s,o)\). \(R(s,o)\) denotes the expected reward for executing option \(o\) in state \(s\). \(P(s' |s,o)\) is the transition probability of \(o\) terminating in state \(s'\) on starting in s after \(\tau\) time steps. \(V_o\) and \(Q_o\) are the value functions for following the policy over the options in \(\mathcal{O}\) for the states in \(S\). The Bellman equations for the value function and action-value function is given by:
Here we will assume the reader has some prior knowledge of action-value and state-value functions, as well as reward functions. Then the Bellman equations are given as follows:

\begin{equation}
    V_\mathcal{O}^* (s)=\max_{o \in  \mathcal{O}_s }⁡[R(s,o)  +\sum_{s'}P(s' \mid s,o) V_\mathcal{O}^* (s')]
\end{equation}

\begin{equation}
    Q_\mathcal{O}^* (s,o) = R(s,o) + \sum_{s'} P(s'\mid s,o)   \max_{o' \in \mathcal{O}_s } ⁡Q_\mathcal{O}^* (s',o')]
\end{equation}

And the SMDP Q-learning update for this action-value function is:

\begin{equation}
    Q_{k+1} (s,o)=(1-\alpha_k ) Q_k (s,o)+\alpha_k [r+ \gamma^\tau.   \max_{o' \in O_{s'}}⁡ Q_k (s',o' )]
\end{equation}


This update is applied upon the termination of \(o\) at state \(s'\) after executing for \(\tau\) time-steps, and \(r\) is the return accumulated during \(o^'\) s execution.  \\



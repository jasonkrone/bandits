\section{Comparison of various frameworks}

The frameworks we have discussed differ in the way sub-tasks are defined as well as how the action-value is learnt
inside and over the sub-problems. In this comparison we consider three factors: Optimality, Abstraction,
and Input Requirements.


\subsection{Optimality}

We distinguish between two types of optimality: recursive optimality and hierarchical optimality.
Recursively optimal frameworks learn at all levels in the hierarchy.
In contrast, hierarchically optimal frameworks learn a policy over skills.
Modular solutions are useful because they allow the policy at the highest level in the hierarchy to operate over longer
time horizons. On the other hand, hierarchical optimality can often generate superior policies by optimizing
all levels of the hierarchy.

The options framework assumes that the policies internal to options are complete and frozen.
Moreover, the aim of the learning process in the context of options is to find an optimal policy
over options. Therefore, this framework produces a hierarchically optimal policy.
Typically, a policy over options is learned using the SMDP variant of the Q-learning algorithm.

The HAM hierarchy is comprised of finite state machines where the call states form the links between levels in the hierarchy.
Q-values (flow of control over the machines) are learnt at the call/action state of each machine.
These values are updated for every state-machine pair rather than for the states inside one machine.
Consequently, the flow of control from one machine to another is optimized rather than the internal transfer between states.
For this reason HAMs are hierarchically optimal and similar to options.

Max-Q decomposes a hierarchy using into tasks and subtasks.
It learns to optimally solve each of these sub-tasks using
value functions and chains these skills together to form one solution.
Hence, it provides a recursively optimal solution.
In short, Max-Q foregoes hierarchical optimality in favor of state abstraction.

Feudal RL defines its hierarchy in terms of managers and sub-managers.
Through information hiding and reward hiding feudal RL allows learning to occur at each level in the hierarchy.
Thus feudal RL is recursively optimal, modular, and quite flexible.
\subsection{Abstraction}

All hierarchical frameworks provide temporal abstraction, which is attained by decomposing the problem
using temporally extended activities. However, not all frameworks allow for state abstraction.
In RL, each state is defined by a set of features. State abstraction removes irrelevant features from
the state to make it as compact. State abstraction is beneficial because it results in computational benefits given
unnecessary details of states eliminated.

Max-Q provides state abstraction by retaining only some relevant variables for each task.
For instance, in the taxi problem that is discussed in \cite{Dietterich},
the destination of the passenger is irrelevant to the driver.
As a result, Max-Q abstracts this detail away for the driver.
Max-Q also provides abstraction through funneling.
Funneling is the process of mapping many states to a smaller subset of states.
Funneling ensures that irrespective of which path is chosen,
the agent always ends in the same subset of terminal states.
In short, Max-Q enables a large degree of abstraction.

In the original formulation of options, Q-learning is done to learn a policy over options.
Unlike Max-Q, the options framework does not get rid of irrelevant features explicitly.
However, some state abstraction can be gained if the options cover most intermediate states
and learning is mainly done to determine when to execute options.
For these reasons options introduce a lesser degree of state abstraction than Max-Q.

The original proposal of HAM does not include any state abstraction.
This is because all states are equally important and learning happens in all of them.

The primary purpose of feudal RL is to provide state abstraction by hiding low level details
of the state space from the upper levels of the hierarchy. Therefore, like Max-Q, feudal RL
also enables a significant state abstraction.

\subsection{Requirements}

Options require complete specification of policy and are applicable only if the policies to subproblems are specified. It just learns the action flow between sub-problems. Hence, it becomes essential to come up with ways to learn the policies within these options.  This requirement of having to learn both these policies may make it result in non-modular structure with more additional time for computing multiple dimensions of policies. 

HAMs also require a complete policy defined before learning the problem. This complete policy decides the representation of states inside a machine. However, unlike Options, HAMs need not learn the best policies for sub-problems before finding the overall best solution. Also, as compared to the other two frameworks, HAMs provide additional details like task hierarchy (machine-to-machine call stack) and amount of effort (time spent in each machine). Hence, they are very expressive. 

Unlike Options and HAMs, MAXQ does not require the complete specification of the policy. It just requires a well-defined stack of sub-tasks. The less input knowledge and requirements in MAXQ makes it the most used and flexible framework among researchers.

\section{Comparison of various frameworks}

The three fundamental frameworks are compared based on three aspects: Optimality, Abstraction and Requirements. 

\subsection{Optimality}
Before contrasting the various basic frameworks of HRL based on optimality. It is important to understand the notion of optimality in this set-up. \\

\textbf{Recursively Optimal} \\
A policy is recursively optimal if the solutions to each of its sub-problems is optimal. Such a solution respects the hierarchy but does not yield the globally optimal solution in all the cases. Given the optimal policies to the sub-problems, a recursively optimal solution is simply the chain of these optimal sub-problems. It brings about modularity, thus, decreasing the learning horizon. Hence, recursively optimal solution might be advantageous in problems with extremely large state space. However, in smaller set-up, it tends to show great deviations from the most optimal solution. \\

\textbf{Hierarchically Optimal} \\
A policy is hierarchically optimal if it is the most optimal amongst all the policies that respect the hierarchy. Such a solution need not be optimal on each of its subproblems. \\

\textbf{Flat Optimal} \\
Unlike the other two optimality’s, a flat solution does not respect hierarchy. It simply yields us the solution which is globally the most optimal. It might not even compute the solutions to the individual subproblems. \\


Consider a big problem T, divided into three smaller sub-problems $T_1$, $T_2$ and $T_3$, with a hierarchy that $T_1$ has to be solved before $T_2$ and $T_2$ before $T_3$. A recursively optimal solution finds the best solutions to each of $T_1$, $T_2$ and $T_3$ and concatenates those best solutions to form the solution to the original problem T. On the other hand, a hierarchically optimal solution finds all the solutions which are formed by solving $T_1$ first, then $T_2$ and $T_3$, and then chooses the most optimal one amongst these solutions. Notice that, this solution need not contain ways which mention the best way of doing the individual subproblems. Lastly, a flat solution just finds the best way of solving $T$. It does not provide any restrictions on the hierarchy. From this example, it is evident that a flat solution always provide the most optimal solution cost-wise, followed by a hierarchically optimal one and then recursive. Despite this pro, flat solutions are neglected as they defy the basic notion of hierarchy, which brings about temporal and state abstraction. Thus, this leaves us with just the two options of hierarchical and recursive optimality. \\

The optimality provided by the three fundamental frameworks of HRL are: options – hierarchical, HAM – hierarchical and MAXQ – recursive.	As we know from the Options sections, policies are learnt over multiple options at just the initial and terminal states and not within an option. This basic layout points out that irrespective of the ways a solution is learnt within an option, the solution is the one which provides the best inter-option policies. Hence, options are hierarchically optimal frameworks. Just like options, HAM’s also apply a SMDP Q-learning. Hence, they also follow a hierarchically optimal framework. In MAXQ, a hierarchy of sub-tasks is defined. It learns to optimally solve each of these sub-tasks and chains them, which is essentially a recursive optimal solution. It gives up on hierarchical optimality for state abstraction. MAXQ prevents sub-tasks from being context dependent (overlook the overall picture) and thus, has compact value functions. However, it pays a price of optimality, which might be negligible for very large state space. \\


\subsection{Abstraction}

Any hierarchical framework will provide temporal abstraction, which is simply dividing the entire problem into temporally extended activities. The distinction between various frameworks are brought about by state abstraction. An abstract state is essentially a state with a fewer state variables. Abstract state tends to keep aside all irrelevant features. In other words, for each subproblem, a state abstraction is provided by identifying the subset of state variables that are relevant. From this definition, it is evident that different world states can map to the same abstract state. Reducing the state variables, reduces the learning time complexity considerably. Different definitions of abstract states can be used for different sub-problems. Both state and temporal abstractions, bring about reusability and reduced computational power. This reduction in computational power is like the one provided by function approximators. \\

MAXQ provides state abstraction by retaining only some relevant variables for each task. For instance, as per Diettrich’s taxi problem, the destination of the passenger is irrelevant to the driver when he is picking him up. This detail is abstracted away from the driver when the driver is executing the pick up sub tasks. MAXQ also provides abstraction through funneling. Funneling is the process of mapping many states to smaller subset of states. For instance, in the real-world subtask of going to Times Square, maps the passenger in any given location to one target location of Times Square. So this makes sure that irrespective of which path we choose, we always reach the same subset of destinations. The removal of irrelevance and the use of funneling, brings about a commendable abstraction in MAXQ. \\

In options, Q-learning occurs only on initial and terminal states, and not on any other states. This distinction between the states can be considered as abstraction. Note that, unlike MAXQ it does not get rid of irrelevance explicitly. However, it reduces the state space by giving a higher weightage to just a few states. \\

Unlike the other frameworks, the original proposal of HAM does not include any abstraction. The reason being that all states are equally important and learning happens in all of them. Grouping of multiple states to machines brings about hierarchy but does not reduce the state space. Abstractions can be explicitly provided by using techniques like three-way decomposition, wherein both the machine and state influence the Q-learning, rather than just the state. This tries to cause machine-level abstraction of states. But still note that general HAM DOES NOT provide state abstraction. \\


\subsection{Expressiveness and Requirements}
Options require complete specification of policy and would learn only if the policies to subproblems are given completely. It just learns the action flow between sub-problems. Hence, it becomes essential to come up with ways to learn the policies of sub-problems. Learning problems making use of options usually incorporate two types of learning: intra-option learning to learn the policies over actions within an option and inter-option learning to learn the policies over the options. This requirement of having to learn both these policies may make it result in non-modular structure with more additional time for computing multiple dimensions of policies. \\

HAM’s also require a complete policy defined before learning the problem. This complete policy decides the representation of states inside a machine. However, unlike Options, HAM’s need not learn the best policies for sub-problems before finding the overall best solution. Also, as compared to the other two frameworks, HAM’s provide additional details like task hierarchy (machine-to-machine call stack) and amount of effort (time spent in each machine). Hence, they are very expressive. \\

Unlike Options and HAMs, MAXQ does not require the complete specification of the policy. It just requires a well-defined stack of sub-tasks. The less input knowledge and requirements in MAXQ makes it the most used framework among researchers. \\

Thus, in all, the three frameworks of HRL can be summarized as given in Table:

% \usepackage[table,xcdraw]{xcolor}
% \documentclass[xcolor=table]{beamer}
\begin{table}[]
\centering
\caption{My caption}
\label{my-label}
\begin{tabular}{|l|l|l|l|}
\hline
\rowcolor[HTML]{EFEFEF} 
                              & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}\textbf{OPTIONS}}                  & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}\textbf{HAM}}                                                         & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}\textbf{MAXQ}}                                              \\ \hline
\textbf{Temporal Abstraction} & Yes                                                                            & Yes                                                                                                               & Yes                                                                                                     \\ \hline
\textbf{State Abstraction}    & Yes                                                                            & No                                                                                                                & \begin{tabular}[c]{@{}l@{}}Yes \\   Provides funneling and explicit removal of irrelevance\end{tabular} \\ \hline
\textbf{Optimality}           & Hierarchical                                                                   & Hierarchical                                                                                                      & Recursive                                                                                               \\ \hline
\textbf{Input Requirements}   & Complete Policy                                                                & Complete Policy                                                                                                   & Does not require,policy to be specified explicitly                                                      \\ \hline
\textbf{Expressiveness}       & \begin{tabular}[c]{@{}l@{}}No explicit display\\   of information\end{tabular} & \begin{tabular}[c]{@{}l@{}}Provides\\   information like hierarchy, time spent and effort explicitly\end{tabular} & Just displays the,notion of hierarchy and nothing else                                                  \\ \hline
\end{tabular}
\end{table}




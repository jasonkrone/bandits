\section{Comparison of various frameworks}

The fundamental frameworks differ in the way a sub-problem is defined and how the Q-value is learnt inside and over the sub-problems. For comparison, three factors are considered: Optimality, Abstraction and the Input Requirements of the three models. 


\subsection{Optimality}

Recursive optimality provides modularity whereas hierarchical optimality yields a policy strictly adhering to  the hierarchy.  Modular solutions reduce the horizon by a large factor, on the other hand, better policies yields higher rewards. 

Options framework is analogous to any other RL framework with the set of options replacing the set of actions. It assumes that the policies internal to options are complete and frozen. A standard SMDP Q-learning is applied at the terminal states to learn the policies over options. As the learning happens over options (hierarchies), this framework produces a hierarchically optimal policy. 

HAM replaces the option with a non-deterministic finite state machine with various states. Multiple call states of machines stacked together forms the hierarchy. Q-values (flow of control over the machines) are learnt at the call/action state of each machine. These values are updated for every state-machine pair, rather than the states inside one machine. In other words, the internal transfer of states inside a machine is not optimized, rather the flow of control from one machine to another is. Thus, the policies over machines in HAMs is hierarchically optimal.  


MAXQ describes a hierarchy of sub-tasks. It learns to optimally solve each of these sub-tasks and chains them to form one solution. Hence, it provides a recursively optimal solution. It gives up on hierarchical optimality for state abstraction. MAXQ prevents sub-tasks from being context dependent (overlook the overall picture) and thus, has compact value functions. However, it pays a price of optimality, which might be negligible for very large state space

Feudal networks encapsulate the sub-managers functionalities and abstracts it away from that of the managers. This abstraction makes sure that the manager is not aware of the fundamental policies inside any of the sub-managers. This lower level hiding of details provides modularity and thus, these networks follow recursive optimality. 


\subsection{Abstraction}

All hierarchical frameworks provide temporal abstraction, which is simply dividing the entire problem into temporally extended activities. However, not all frameworks obey state abstraction. In RL, each state is defined by a set of features (variables). State abstraction removes irrelevant features from the state to make it as compact and robust as possible. State Abstraction results in huge computational benefits as the unwanted details from states of subproblems are eliminated completely. 

MAXQ provides state abstraction by retaining only some relevant variables for each task. For instance, as per Diettrich taxi problem, the destination of the passenger is irrelevant to the driver when he is picking him up. This detail is abstracted away from the driver when he is executing the pick-up sub task. MAXQ also provides abstraction through funneling. Funneling is the process of mapping many states to smaller subset of states. It makes sure that irrespective of which path is chosen, the agent always ends in the same subset of terminal states. The removal of irrelevance and the use of funneling, brings about commendable abstraction in MAXQ.

In options, Q-learning occurs only at the initial and terminal states. It thus, provides multiple classes of states. This distinction between the states can be considered as abstraction. Note that, unlike MAXQ it does not get rid of irrelevance explicitly. However, it reduces the state space by giving a higher weightage to just a few states. 

The original proposal of HAM does not include any abstraction. The reason being that all states are equally important and learning happens in all of them. Grouping of multiple states to machines brings about hierarchy but does not reduce the state space. Unlike HAMs, the primary purpose of feudal networks was to abstract away multiple states and hide lower level details of one state from the other. Thus, it also provides state abstraction. 


\subsection{Requirements}

Options require complete specification of policy and are applicable only if the policies to subproblems are specified. It just learns the action flow between sub-problems. Hence, it becomes essential to come up with ways to learn the policies within these options.  This requirement of having to learn both these policies may make it result in non-modular structure with more additional time for computing multiple dimensions of policies. 

HAMs also require a complete policy defined before learning the problem. This complete policy decides the representation of states inside a machine. However, unlike Options, HAMs need not learn the best policies for sub-problems before finding the overall best solution. Also, as compared to the other two frameworks, HAMs provide additional details like task hierarchy (machine-to-machine call stack) and amount of effort (time spent in each machine). Hence, they are very expressive. 

Unlike Options and HAMs, MAXQ does not require the complete specification of the policy. It just requires a well-defined stack of sub-tasks. The less input knowledge and requirements in MAXQ makes it the most used and flexible framework among researchers.

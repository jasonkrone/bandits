\section{Comparison of various frameworks}

The fundamental frameworks differ in the way a sub-problem is defined and how the Q-value is learnt inside and over the sub-problems. For comparison, three factors are considered: Optimality, Abstraction and the Input Requirements of the three models. Optimality decides between the trade-off between modularity v/s optimized cost. Abstraction checks if the fundamental representation of the state is abstract or not. Input requirements mentions the parameters necessary for executing the runs of the framework and highlights upon the flexibility of models. 

\subsection{Optimality}


In the Options framework, the SMDP Q-learning learns the policies over multiple options at the terminal states of the options. It is assumed here that the policies with an option are complete and frozen i.e. they do not change over stages. On the other hand, if intra-option learning is used, then the values can be computed even at non-terminal stages. This basic layout points out that irrespective of the ways a solution is learnt within an option, the solution always provides the best inter-option policies. Hence, options are hierarchically optimal. Despite the final cost of the reward being higher than the flat one, options tend to have a more optimized reward as compared to MAXQ, its recursive counterpart. 

HAMs work like options with just the difference that they make use of finite state machines instead of options. The HAM Q-learning agent keeps track of quantities like the current environment, state, machine, total accumulated reward and discount to update the Q-value in each iteration.  The values are updated for every state-machine pair, rather than just the states inside one machine. This is similar to that of Options. Hence, they also provide hierarchical optimality. 
 
MAXQ describes a hierarchy of sub-tasks. It learns to optimally solve each of these sub-tasks and chains them, which is essentially a recursive optimal solution. It gives up on hierarchical optimality for state abstraction. MAXQ prevents sub-tasks from being context dependent (overlook the overall picture) and thus, has compact value functions. However, it pays a price of optimality, which might be negligible for very large state space

Feudal networks encapsulate the sub-managers functionalities and abstracts it away from that of the managers. This abstraction makes sure that the manager is not aware of the fundamental policies inside any of the sub-managers. This lower level hiding of details provides modularity and thus, these networks follow recursive optimality. 


\subsection{Abstraction}

Any hierarchical framework will provide temporal abstraction, which is simply dividing the entire problem into temporally extended activities. The distinction between various frameworks are brought about by state abstraction. An abstract state is essentially a state with a fewer state variables. Abstract state tends to keep aside all irrelevant features. In other words, for each subproblem, a state abstraction is provided by identifying the subset of state variables that are relevant. From this definition, it is evident that different world states can map to the same abstract state. Reducing the state variables, reduces the learning time complexity considerably. Different definitions of abstract states can be used for different sub-problems. Both state and temporal abstractions, bring about reusability and reduced computational power. This reduction in computational power is like the one provided by function approximators. 

MAXQ provides state abstraction by retaining only some relevant variables for each task. For instance, as per Diettrich taxi problem, the destination of the passenger is irrelevant to the driver when he is picking him up. This detail is abstracted away from the driver when the driver is executing the pick-up sub tasks. MAXQ also provides abstraction through funneling. Funneling is the process of mapping many states to smaller subset of states. For instance, in the real-world subtask of going to Times Square, maps the passenger in any given location to one target location of Times Square. So this makes sure that irrespective of which path we choose, we always reach the same subset of destinations. The removal of irrelevance and the use of funneling, brings about a commendable abstraction in MAXQ.

In options, Q-learning occurs only on initial and terminal states, and not on any other states. This distinction between the states can be considered as abstraction. Note that, unlike MAXQ it does not get rid of irrelevance explicitly. However, it reduces the state space by giving a higher weightage to just a few states. 

Unlike the other frameworks, the original proposal of HAM does not include any abstraction. The reason being that all states are equally important and learning happens in all of them. Grouping of multiple states to machines brings about hierarchy but does not reduce the state space. Abstractions can be explicitly provided by using techniques like three-way decomposition, wherein both the machine and state influence the Q-learning, rather than just the state. This tries to cause machine-level abstraction of states. But still note that general HAM DOES NOT provide state abstraction. 

Feudal networks primary purpose was to abstract away multiple states and hide lower level details of one state from the other. Thus, it also provides state abstraction. 

\subsection{Requirements}

Options require complete specification of policy and would learn only if the policies to subproblems are given completely. It just learns the action flow between sub-problems. Hence, it becomes essential to come up with ways to learn the policies of sub-problems. Learning problems making use of options usually incorporate two types of learning: intra-option learning to learn the policies over actions within an option and inter-option learning to learn the policies over the options. This requirement of having to learn both these policies may make it result in non-modular structure with more additional time for computing multiple dimensions of policies. 

HAMs also require a complete policy defined before learning the problem. This complete policy decides the representation of states inside a machine. However, unlike Options, HAMs need not learn the best policies for sub-problems before finding the overall best solution. Also, as compared to the other two frameworks, HAMs provide additional details like task hierarchy (machine-to-machine call stack) and amount of effort (time spent in each machine). Hence, they are very expressive. 

Unlike Options and HAMs, MAXQ does not require the complete specification of the policy. It just requires a well-defined stack of sub-tasks. The less input knowledge and requirements in MAXQ makes it the most used framework among researchers.


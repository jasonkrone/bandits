\section{Comparison of various frameworks}

The fundamental frameworks differ in the way a sub-problem is defined and how the Q-value is learned inside and over the sub-problems. For comparison, three factors are considered: Optimality, Abstraction and the Input Requirements of the four models. Optimality decides the trade-off between modularity and optimized cost. Abstraction checks if the fundamental representation of the state is abstract or not. Input requirements are parameters necessary for executing the runs of the framework and highlights upon the flexibility of models.

\subsection{Optimality}

In the Options framework, the SMDP Q-learning learns the policies over multiple options at the terminal states of the options. It is assumed here that the policies within an option are complete and frozen i.e. they do not change over stages. On the other hand, if intra-option learning is used, then the values can be computed at non-terminal stages. This basic layout points out that irrespective of the ways solutions are learned within options the solution always provides the best intra-option policy, making options hierarchically optimal. Despite the final cost of the reward being higher than the flat optimality, options tend to have better reward than MAXQ, the recursively optimal counterpart.

For our purposes we will think of HAMs as similar to Options, but with finite state machines instead of options. The HAM Q-learning agent keeps track of quantities like the current environment, state, machine, total accumulated reward and discount to update the Q-value in each iteration. The values are updated for every state-machine pair rather than the states inside one machine, similar to Options. Hence, they also provide hierarchical optimality.

MAXQ describes a hierarchy of sub-tasks. It learns to optimally solve each of these sub-tasks and chains them, providing a recursively optimal solution. MAXQ prevents sub-tasks from being context dependent (overlook the overall picture) which results in its compact value functions, but pays a price in optimality.

Feudal networks encapsulate the sub-manager functionalities and abstracts it away from that of the managers. This abstraction makes sure that the manager is not aware of the fundamental policies inside any of the sub-managers. This lower level hiding of details provides modularity and results in recursive optimality.

\subsection{Abstraction}

All hierarchical frameworks provide temporal abstraction, but the distinction between these frameworks lies in state abstraction. A state abstraction is a state with a fewer variables and hide irrelevant features. In other words, for each subproblem a state abstraction is provided by identifying the subset of state variables that are relevant. From this definition it is evident that different world states can map to the same abstract state. Reducing the state variables reduces the learning time complexity considerably. Different definitions of abstract states can be used for different sub-problems. Both state and temporal abstractions bring about reusability and reduced computational power. This reduction in computational power is like the one provided by function approximators.

MAXQ provides state abstraction by retaining only some relevant variables for each task. For instance, as per the taxi problem \cite{Dietterich}, the destination of the passenger is irrelevant to the driver when he is picking a passenger up. This detail is abstracted away from the driver when the driver is executing the pick-up sub task. MAXQ also provides abstraction through funneling. Funneling is the process of mapping many states to a smaller subset of states. For instance, in the real world subtask of going to Times Square maps the passenger in any given location to one target location of Times Square. This ensures that irrespective of which path we choose, we always reach the same subset of destinations. The removal of irrelevance and the use of funneling brings about a commendable abstraction in MAXQ.

In options, Q-learning occurs only on initial and terminal states. This distinction between states can be considered an abstraction. Note that unlike MAXQ it does remove irrelevance explicitly. However, it reduces the state space by giving a higher weight to just a few states.

Unlike the other frameworks, the original proposal of HAM does not include any abstraction. The reason being that all states are equally important and learning happens in all of them. Grouping multiple states to machines brings about hierarchy but does not reduce the state space. Abstractions can be explicitly provided using techniques like three-way decomposition, wherein both the machine and state influence Q-learning rather than just the state. This tries to cause machine-level abstraction of states, but note that the general HAM does not provide state abstraction.

Feudal networks primary purpose was to abstract away multiple states and hide lower level details of one state from others. Thus providing state abstraction.

\subsection{Requirements}

Options require complete specification of policy and would learn only if the policies to subproblems are given completely. It just learns the action flow between sub-problems, making it essential to come up with ways to learn the policies of sub-problems. Learning problems with options usually incorporates two types of learning: intra-option learning to learn the policies over actions within an option and inter-option learning to learn the policies over the options. This requirement of having to learn both these policies may make it result in non-modular structure with more additional time for computing multiple dimensions of policies.

HAMs also require a complete policy defined before learning the problem. This complete policy decides the representation of states inside a machine. Unlike Options, HAMs need not learn the best policies for sub-problems before finding the overall best solution. Also, as compared to the other two frameworks HAMs provide additional details like task hierarchy (machine-to-machine call stack) and amount of effort (time spent in each machine). Hence, they are very expressive.

Unlike Options and HAMs, MAXQ does not require the complete specification of the policy. It just requires a well-defined stack of sub-tasks.


\section{Comparison of various frameworks}

The frameworks we have discussed differ in the way sub-tasks are defined as well as how the action-value is learnt
inside and over the sub-problems. In this comparison we consider three factors: Optimality, Abstraction,
and Input Requirements.


\subsection{Optimality}

We distinguish between two types of optimality: recursive optimality and hierarchical optimality.
Recursively optimal frameworks learn at all levels in the hierarchy.
In contrast, hierarchically optimal frameworks learn a policy over skills.
Modular solutions are useful because they allow the policy at the highest level in the hierarchy to operate over longer
time horizons. On the other hand, hierarchical optimality can often generate superior policies by optimizing
all levels of the hierarchy.

The options framework assumes that the policies internal to options are complete and frozen.
Moreover, the aim of the learning process in the context of options is to find an optimal policy
over options. Therefore, this framework produces a hierarchically optimal policy.
Typically, a policy over options is learned using the SMDP variant of the Q-learning algorithm.

The HAM hierarchy is comprised of finite state machines where the call states form the links between levels in the hierarchy.
Q-values (flow of control over the machines) are learnt at the call/action state of each machine.
These values are updated for every state-machine pair rather than for the states inside one machine.
Consequently, the flow of control from one machine to another is optimized rather than the internal transfer between states.
For this reason HAMs are hierarchically optimal and similar to options.

Max-Q decomposes a hierarchy using into tasks and subtasks.
It learns to optimally solve each of these sub-tasks using
value functions and chains these skills together to form one solution.
Hence, it provides a recursively optimal solution.
In short, Max-Q foregoes hierarchical optimality in favor of state abstraction.

Feudal RL defines its hierarchy in terms of managers and sub-managers.
Through information hiding and reward hiding feudal RL allows learning to occur at each level in the hierarchy.
Thus feudal RL is recursively optimal, modular, and quite flexible.
\subsection{Abstraction}

All hierarchical frameworks provide temporal abstraction, which is simply dividing the entire problem into temporally extended activities. However, not all frameworks obey state abstraction. In RL, each state is defined by a set of features (variables). State abstraction removes irrelevant features from the state to make it as compact and robust as possible. State Abstraction results in huge computational benefits as the unwanted details from states of subproblems are eliminated completely. 

MAXQ provides state abstraction by retaining only some relevant variables for each task. For instance, as per Diettrich taxi problem, the destination of the passenger is irrelevant to the driver when he is picking him up. This detail is abstracted away from the driver when he is executing the pick-up sub task. MAXQ also provides abstraction through funneling. Funneling is the process of mapping many states to smaller subset of states. It makes sure that irrespective of which path is chosen, the agent always ends in the same subset of terminal states. The removal of irrelevance and the use of funneling, brings about commendable abstraction in MAXQ.

In options, Q-learning occurs only at the initial and terminal states. It thus, provides multiple classes of states. This distinction between the states can be considered as abstraction. Note that, unlike MAXQ it does not get rid of irrelevance explicitly. However, it reduces the state space by giving a higher weightage to just a few states. 

The original proposal of HAM does not include any abstraction. The reason being that all states are equally important and learning happens in all of them. Grouping of multiple states to machines brings about hierarchy but does not reduce the state space. Unlike HAMs, the primary purpose of feudal networks was to abstract away multiple states and hide lower level details of one state from the other. Thus, it also provides state abstraction. 


\subsection{Requirements}

Options require complete specification of policy and are applicable only if the policies to subproblems are specified. It just learns the action flow between sub-problems. Hence, it becomes essential to come up with ways to learn the policies within these options.  This requirement of having to learn both these policies may make it result in non-modular structure with more additional time for computing multiple dimensions of policies. 

HAMs also require a complete policy defined before learning the problem. This complete policy decides the representation of states inside a machine. However, unlike Options, HAMs need not learn the best policies for sub-problems before finding the overall best solution. Also, as compared to the other two frameworks, HAMs provide additional details like task hierarchy (machine-to-machine call stack) and amount of effort (time spent in each machine). Hence, they are very expressive. 

Unlike Options and HAMs, MAXQ does not require the complete specification of the policy. It just requires a well-defined stack of sub-tasks. The less input knowledge and requirements in MAXQ makes it the most used and flexible framework among researchers.

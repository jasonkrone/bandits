\section{Feudal Reinforcement Learning [Jason]}


\textbf{Description} \\
Feudal Reinforcement Learning is modeled after a feudal fiefdom where the hierarchy consists
of managers and sub-managers. At each intermediate level of the hierarchy an agent performs
either a primitive action, belonging to the action set $A$ associated with the environment, or
a special action, which passes control to a sub-manager. When a special action is selected, control is
returned to the manager once the state changes at the managerial level. This approach relies on two
forms of abstraction namely Reward Hiding and Information Hiding as described below.

\begin{enumerate}
    \item Reward Hiding: Managers must reward sub-managers for doing their bidding whether or not this
          satisfies the commands of super-managers.
    \item Information Hiding: Managers only need to know the state of the system at the granularity of
          their own choices of tasks. The task of each agent (manager / sub-manager) is only known to that
          agent. Managers do need to know satisfaction conditions for tasks they set.
\end{enumerate}

This structure brings with it both strengths and weaknesses. We shall begin by discussing the strengths.

\textbf{Strengths}
\begin{itemize}
    \item Reward Hiding enables low level sub-managers to learn from accomplishing sub-tasks early in training
          because the success of these sub-managers is not dependent on accomplishing the highest level goal.
    \item Exploration can be done with greater control over granularity by executing broad exploration from high levels
          in the hierarchy and directed exploration from low levels in the hierarchy.
    \item The feudal structure lends itself to reducing the state-space covered by a given manager.
          This makes it easier for agents to calculate which actions to take a learning using value function approximation.
\end{itemize}

The abstractions mentioned above also create weaknesses.

\textbf{Weaknesses}
\begin{itemize}
    \item The success of Feudal Reinforcement learning assumes there is a natural hierarchical division of the environment
          and, ideally, the state space.
    \item Information Hiding can create inefficiency by forcing sub-managers to learn to satisfy tasks that are irrelevant to
          accomplishing the highest level goal.
\end{itemize}

\textbf{Flexibility} \\
Feudal reinforcement learning is quite flexible. Sub-tasks delegated to sub-managers can be learned as is done in
FeUdal Networks for Hierarchical Reinforcement Learning. In addition, there can be an arbitrary number of levels in the
hierarchy. The main constraint is that the environment must be early decomposed using a hierarchical structure, which
must be specified.


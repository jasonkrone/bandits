\section{Ongoing Challenges}
An issue central to Reinforcement Learning research is establishing common benchmarks
that can be consistently used to compare different algorithms. The difficulty of
doing this is especially pronounced for hierarchical reinforcement learning.
The recent works discussed in this paper, excluding those on dialogue management,
experiment on seven different environments (assuming that the suite of Atari games
constitutes one environment). Consequently, only following pairs of papers
can be directly compared to each other using the published results.

\begin{enumerate}
    \item Option-critic, Hybrid Reward Architecture for Reinforcement Learning, and
          FeUdal Networks
    \item Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction
          and Intrinsic Motivation and FeUdal Networks
\end{enumerate}

Moreover, some of the published models can not be easily applied to new environments because
they use goals or pseudo rewards that are environment dependent. It is also the case
that published papers do not compare their hierarchical reinforcement learning system to
other hierarchical reinforcement learning systems. None of the recent papers in this review
used another hierarchical reinfocement learning system as a baseline, which makes it difficult
to assess the quality of the method.

A first step toward overcoming these challenges would be to make open source implementations
of all the state-of-the-art hierarchical reinforcement learning methods available in a single repository.
This suggestion is inspired by the baselines repository released by OpenAI, which provides TensorFlow
implementations of the most popular Deep Reinforcement Learning algorithms.



\section{Introduction}

Reinforcement learning (RL) is the area of machine learning concerned with an agent interacting with an environment with the objective of learning the optimal behavioral policy to maximize a reward. An environment is usually defined by a set of states, transition probabilities between states, and reward functions for transitions. When the transition function and reward functions are known are known, dynamic programming approaches like value iteration can be used to determine an optimal policy. Otherwise, the agent must attempt to learn about rewards through interacting with the environment, by using the RMAX algorithm for example. In both of these cases, however, these algorithms do not scale well when the state space is large. 

In the approaches described above, standard RL approaches run into problems such as sparse feedback or long horizon problems. In the case of sparse rewards, only a few states realize reward observations. This makes exploration of states as defined by value iteration or RMAX inefficient. Long horizon problems are those that require optimization over serveral time steps, and observations at the state level are inefficient in optimizing policy for these type of problems. Hierarchical approaches to this problem aim to take advantage of activities that have common sub-components. For example, in Montezuma's revenge, a common task is to navigate around an obstacle. Instead of learning navigation at a state level, it would be more useful to have a subpolicy that guides the player around the obstacle over several timesteps.  Hierarchical RL aims to learn these sub-tasks and then combine them to solve the original problem. This might be done through temporally extended activites where decisions aren't required at every time step. Instead, the highest level of the hierarchy chooses subpolicies that last several timesteps. Actions during these timesteps are governed by the subpolicy rather than at the top of the hierarchy.   

This paper is a review of different several hierarchical reinforcement learning methods. We first define Markov Decision Processes(MDP) and Semi-Markov Decision Processes(SMDP) which is the motivating problem of RL. We then describe several of the classical strategies: options, hierarchical abstract machines (HAM), feudal RL, and MAXQ. We explain how each of these algorithms work, how flexible they are in learning, how they use temporal or state abstraction to provide more scalable approaches to solving MDPs, and how they are advantageous over the other approaches. 

We then describe more modern modern approaches. These approaches, rather than developing additional frameworks like the ones described above, show how advances in computing allowed better performance when using established methods and allowed applications to more complex problems such as dialog agents or Atari games. Many of the modern approaches have used neural networks and deep learning to more accurately approximate reward functions or latent variables that arise from using the methods above. While this is not an exhaustive review of all work done in modern hierarchical RL, we intend this paper to clearly present some classical approaches and give concrete examples of how these approaches can be extended.  


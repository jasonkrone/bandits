\section{Introduction}

Reinforcement learning (RL) is the area of machine learning concerned with an agent interacting with an environment with the objective of learning the optimal behavioral policy to maximize a reward. An environment is usually defined by a set of states, transition probabilities between states, and reward functions for transitions. When the transition function and reward functions are known are known, dynamic programming approaches like value iteration can be used to determine an optimal policy. Otherwise, the agent must attempt to learn about rewards through interacting with the environment, by using the RMAX algorithm for example. In both of these cases, however, these algorithms do not scale well when the state space is large. 

In the approaches described above, an agent typically needs to explore the entire state space to approximate transition functions or reward functions. As the state space grows, standard RL approaches run into difficulties such as sparse feedback  (where the agent receives feedback in only a few of the possible states of the system) or  long horizon problems (where an optimal policy must be found over many time steps).  Hierarchical approaches to this problem aim to take advantage of activities that have common sub-components. For example, in dialog agents, a common sub-task is guiding a user through the payment procedure (booking flights, hotels, cars would require similar procedures to pay for instance). Hierarchical RL aims to learn these sub-tasks and then combine them to solve the original problem. This might be done through temporally extended activites where decisions aren't required at every time step, but rather sub-policies are chosen at some time-steps and are allowed to run to completion. No decisions are rquired while the sub-policy runs.   

This paper is a review of different several hierarchical reinforcement learning methods. We first define Markov Decision Processes(MDP) and Semi-Markov Decision Processes(SMDP) which is the motivating problem of RL. We then describe several of the classical strategies: options, hierarchical abstract machines (HAM), feudal RL, and MAXQ. We explain how each of these algorithms work, how flexible they are in learning, how they use temporal or state abstraction to provide more scalable approaches to solving MDPs, and how they are advantageous over the other approaches. 

We then describe more modern modern approaches. These approaches, rather than developing additional frameworks like the ones described above, show how advances in computing allowed better performance when using established methods and allowed applications to more complex problems such as dialog agents or Atari games. Many of the modern approaches have used neural networks and deep learning to more accurately approximate reward functions or latent variables that arise from using the methods above. While this is not an exhaustive review of all work done in modern hierarchical RL, we intend this paper to clearly present some classical approaches and give concrete examples of how these approaches can be extended.  


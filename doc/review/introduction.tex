\section{Introduction}

Reinforcement learning (RL) is the area of machine learning concerned with an agent interacting with an environment with the objective of learning the optimal behavioral policy to maximize a reward. When information about the environment and the rewards are known, dynamic programming approaches like value iteration can be used to determine an optimal policy. Otherwise, the agent must attempt to learn about rewards through interacting with the environment, by using the RMAX algorithm for example. In both of these cases, however, these algorithms do not scale well when the state space is large. 

In particular, long horizon problems are difficult to solve using standard RL approaches. Hierarchical approaches to this problem aim to take advantage of trajectories that have common sub-components. Hierarchical RL aims to learn these sub-tasks and then combine them to solve the original problem. This might be done through temporally extended activites where decisions aren't required at every time step, but rather choosing activities that follow their own policies over several time steps until they terminate. 

This paper is a review of different several hierarchical reinforcement learning methods. We first define Markov Decision Processes(MDP) and Semi-Markov Decision Processes(SMDP) which is the motivating problem of RL. We then describe several of the classical strategies: options, hierarchical abstract machines (HAM), feudal RL, and MAXQ. We explain how each of these algorithms work, how much they learn, how they use temporal or state abstraction to provide more scalable approaches to solving MDPs, and how they are advantageous over the other approaches. 

We then describe more modern modern approaches. 


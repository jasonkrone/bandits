\section{Timeline and Overview}

\subsection{Feudal Reinforcement Learning}

\par Feudal Reinforcement Learning was developed by Dayan and Hinton in 1992 to resolve issues seen throughout reinforcement learning; temporal resolution, division-and-conquest, exploration, and structural generalisation. To do so, they introduce a hierarchy of managers and submanagers. At any given level of the hierarchy, a manager only has two instruments of control over sub-managers at any given time, they can choose amongst their submanagers and set them to complete subtasks. Only at the lowest level can managers interact with the environment through primitive actions.

\par This hierarchy resolves the issue of temporal resolution because high level managers leave sub-sub-managers to do actual work. It also allows exploration to be done non-uniformly by allowing managers to decide where in the state space the most reward will be found, then send sub-managers there directly.

\par In a feudal system each sub-manager acts to maximize its expected reward. This means that sub-managers at level two can reward sub-sub-managers at level three even if it does not satisfy the commands of super-managers at level one. However if the sub-manager at level two does not eventually complete the task assigned from level one, it will not receive any reward. Similarly, managers only need to know the state necessary to complete the task assigned to them. Managers need to know the cost to the system of achieving a goal and the satisfaction conditions for the task. 

\par The most important advantage to Feudal learning is the reduction in state space. In a system that is constrained to little computational power a reduced state space for any sub-manager would allow the system to learn approximate value functions. This comes at the cost of giving sub-managers no context for their tasks and possibly learning to satisfy sub-tasks irrelevant to the overall goal.

\subsection{Hierarchies of Abstract Machines}

\par Another approach to hierarchical reinforcement learning is Hierarchical Abstract Machine developed by Ronald Parr and Stuart Russell in 1997. In the HAM approach to hierarchical reinforcement learning the programmer designs finite state automata called abstract machines. The abstract machines can be made into a hierarchy by allowing one to call another as a sub-routine. Each machine can be thought of as a partial policy with its own states and actions, thereby providing both state and temporal abstraction.

\par A machine for a HAM is then defined by its set of states, transition function, and a start function to determine the initial state. The four machine states are as follows:

\begin{outline}
\1 Action - executes an action in the environment
\1 Call - execute another machine as a subroutine
\1 Choice - nondeterministically select the next machine state
\1 Stop - end the executionof the machine and return control to the previous call state
\end{outline}

\par One unique advantage to abstract machines is the independence of each machine. Because each machine is independent, several machines can be run concurrently. Unfortunately HAMs require a complete policy in that all possible states must be encoded in the abstract machine.

\subsection{Options}

\par One of the most popular hierarchical reinforcement learning frameworks is the Options framework created by Richard Sutton and Satinder Singh in 1999. Options aim to combine the advantages of both MDPs and SMDPs into a discrete time SMDP by taking temporally extended actions with well-defined policies.

\par Options can be defined by three components: a policy, a termination condition, and an initiation condition. By this definition, options only apply to MDPs, in order to generalize to SMDPs an option's policy must depend on all prior events since the option was initiated. This way termination conditions can include some form of a timeout.

\par The options framework also provides a faster learning rate during the initial stages of training because of intra-option learning. Through intra-option methods, an option's policy can be updated even if that option is not executed until completion. This is a form of off-policy learning methods described by Sutton and Barto.

\par Working with options also has some downsides as well. First, it does not necessarily simplify a complex MDP, but just groups some primitive actions together in the form of an option. Also, the options framework requires the complete specification of policies.

\subsection{MAX Q}

\par The Max-Q approach to hierarchical reinforcement learning was developed by Dietterich in 2000. Similar to Feudal reinforcement learning, the Max-Q method breaks tasks down into a hierarchy of tasks and subtasks, but does so without adding extra state which can happen with special cases of HAMs and semi-markov options.

\par In Max-Q the MDP is decomposed into smaller MDPs by hand such that the smaller MDPs are independent of one another. This way, the MDP is turned into a directed acyclic graph. Each subtask then has its own completion state, and completion value $C$. With this generalization the value function can be decomposed as follows:

\begin{math}
V^{\pi}(i, s) =                                                                    
\begin{cases}                                                                      
V^{\pi}(a, s) + c^{\pi}(i, s, a), & \text{if i is composite}\\                     
\Sigma_{s'}P(s'|s, i)R(s'|s, i), & \text{if i is primitive}                        
\end{cases}
\end{math}

\par Where $c^{\pi}$ is the completion value, realized when the chosen subtask is completed.

\par Decomposition of the value function is the primary contribution of the Max-Q framework. In addition Max-Q provides recursive optimality, which allows subtasks to reach goals independent of the needs of parent tasks. It also allows subtasks to share experience and learning. However, because these tasks are occurring in a stochastic environment the recursively optimal solution for a subtask may not be the optimal solution for the parent task.
\subsection{MAX-Q}
Max-Q reinforcement learning \cite{Dietterich} is very similar to both feudal reinforcement learning and
HAMs. Similar to feudal RL, Max-Q breaks a goal into a hierarchy of tasks
and subtasks (similar to managers and submanagers). Though these tasks and
subtasks must be defined by the programmer there has been some research into
automatic discovery of tasks via landmark states and bottleneck discovery.

Like a HAM, the Max-Q framework decomposes the task without increasing
the state space. What separates the Max-Q framework from HAM and feudal
RL is that Max-Q does this by decomposing the value function.
In Max-Q, the value function takes the following form:

\begin{math}
V^{\pi}(i, s) =
\begin{cases}
V^{\pi}(a, s) + c^{\pi}(i, s, a), & \text{if i is composite}\\
\Sigma_{s'}P(s'|s, i)R(s'|s, i), & \text{if i is primitive}
\end{cases}
\end{math}

where $c^{\pi}$ is the completion value, which is realized when the chosen
subtask is completed.

Max-Q learning introduces recursive optimality, which allows subtasks to reach
their goals independent of the needs of the parent task and depending on the
decomposition of subtasks, related subtasks can share experience. Unfortunately,
a recursively optimal solution is not necessarily globally optimal. Also
the decomposition must be done manually and poor decomposition can lead
to unexpected results.

Because Max-Q learning decomposes tasks in the same way as feudal RL, it is equally as
flexible as feudal RL.


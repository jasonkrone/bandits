\subsection{MAX-Q}
Max-Q reinforcement learning \cite{Dietterich} is very similar to both feudal reinforcement learning and
HAMs. Similar to feudal RL, Max-Q breaks a goal into a hierarchy of tasks
and subtasks (similar to managers and sub-managers). Though these tasks and
subtasks must be defined by the programmer there has been some research into
automatic discovery of tasks via landmark states and bottleneck discovery.

Like a HAM, the Max-Q framework decomposes the task without increasing
the state space. What separates the Max-Q framework from HAM and feudal
RL is that Max-Q does this by decomposing the value function.
In Max-Q, the value function takes the following form:

\begin{math}
V^{\pi}(i, s) =
\begin{cases}
V^{\pi}(a, s) + c^{\pi}(i, s, a), & \text{if i is composite}\\
\Sigma_{s'}P(s'|s, i)R(s'|s, i), & \text{if i is primitive}
\end{cases}
\end{math}

where $c^{\pi}$ is the completion value, which is realized when the chosen
subtask is completed.

Max-Q allows subtasks to reach their goals independent of the needs of the parent task.
In this sense Max-Q is a recursively optimal framework. In addition, intelligent decompositions
of the value function can allow related subtasks to share experience. For example,
moving north, south, east, and west are all tasks, which can experience.


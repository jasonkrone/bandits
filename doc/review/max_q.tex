\subsection{MAX-Q}
Max-Q reinforcement learning \cite{Dietterich} is very similar to both feudal learning and
HAMs. Similar to feudal learning, Max-Q breaks a goal into a hierarchy of tasks
and subtasks (similar to managers and submanagers). Though these tasks and
subtasks must be defined by the programmer there has been some research into
automatic discovery of tasks via landmark states and bottleneck discovery.

Like a HAM, the Max-Q framework decomposes the goal task without increasing
the state space. What separates the Max-Q framework from HAM and feudal
learning is the decomposition of the value function. In Max-Q learning, the
value function takes the following form.

\begin{math}
V^{\pi}(i, s) =
\begin{cases}
V^{\pi}(a, s) + c^{\pi}(i, s, a), & \text{if i is composite}\\
\Sigma_{s'}P(s'|s, i)R(s'|s, i), & \text{if i is primitive}
\end{cases}
\end{math}

where $c^{\pi}$ is the completion value, which is realized when the chosen
subtask is completed.

Max-Q learning introduces recursive optimality, which allows subtasks to reach
their goals independent of the needs of the parent task and depending on the
decomposition of subtasks, related subtasks can share experience. Unfortunately,
a recursively optimal solution is not necessarily globally optimal. Also
the decomposition must be done manually, and a poor decomposition could lead
to unexpected results.

Because Max-Q learning decomposes tasks in the same way as feudal learning, it is no more flexible in application.


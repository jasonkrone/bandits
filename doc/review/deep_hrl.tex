\section{Recent Works on Hierarchical Reinforcement Learning}
Note: for the purpose of this section we assume that recent works are papers released in 2016 or 2017.
To a large degree recent work on hierarchical reinfocement learning focusses on leveraging recent advances in
Deep Learning to augment the foundational hierarchical methods, which we described in the previous section.
Consequently, recent papers are less theoretical and focus on techniques to improve the performance
of existing methods when using neural networks as well as the application hirearchical reinforcement learning
to new, more complex environments, such as the Atari suite of games.
This section discusses the contribution of eight recent works. While undoubtably this list leaves out a number
of recent hierarchical papers, it is our hope that the works selected provide a balanced view of the recent
research direction. We organize this section by grouping papers with similar contributions, and discussing
these groups roughly in order of most to least theoretical.

Before diving into the contributions made in these recent works, it is worth noting which of the frameworks
(options, HAMs, Feudal RL, MaxQ) are used most often as the foundation for this new research.
Options are by far the most popular base for these recent methods; specifically, seven of the eight
papers that we discuss utilize the options framework. In contrast, only one paper bases their work
on Feudal Reinforcement learning, and no papers extend HAMs or MaxQ. Although the reason for this is
not entirely clear, our view is that the options framework was more consistantly developed in the interum
between its conception in 1999 and 2015, which made it more appealing to build upon. Moreover, the options
framework is more intuitive than HAM or MaxQ because an option esentially functions as a sub-policy that is
selected like an action, and it is therefore quite easy to understand options given knolwedge of MDPs and policies.

The most theoretical work is [option critic], which extends options to allow for end-to-end learning
of internal policies and termination conditions for a given number of options using
linear or non-linear function approximators. The primary contribution of this work is the proof
of policy gradient theorems for options, which allow for the end-to-end learning of options.
This is a significant development of the options framework because, to the best of our knowledge,
all applications of Deep Learning to options aside from this paper have required internal policies
and termination conditions to be pre-trained or pre-defined. While, the end-to-end learning of options
is not yet prevelent in the literature, it is a logical extension, which gives superior performance, and
will likely gain traction going forward. It is worth noting that one critism of this work,
made in [Abiel], is that the options learned may be overly specific to the task at hand and not transfer well.

Although no other works have learned the internal policies and termination conditions
for the options they use, both [HDRL Integrating Temporal and Intrisinc] and
[Stochastic NN] work to reduce the amount of hand engineering required
to use options. [Stochastic NN] focuses on learning options in an unsupervised manner
during a separate pre-training phase using a flexable proxy reward.
Specifically, [Stochastic NN] found that Stochastic neural networks coupled with
a mutual information regularizer and a proxy reward, such as the speed of a
robot for the continuous control setting, can reliably learn internal policies for
options with minimal supervision. In contrast,
[HDRL Integrating Temporal and Intrisinc] jointly learns both the policy over options
as well as the internal policy used when executing the options. Both papers choose to
use a single neural network to approximate all internal option policies. They do this
by conditioning the network on a goal $g$, which can be thought of as selecting the
appropriate internal option for the network to approximate. Using a single network
to approximate all internal option policies has the benifits of sharing knowledge
across similar tasks as well as allowing space complexity to remain constant as
the number of options grow.

Similarly, the model used in [FeUdal Networks] learns both sub-manager policies and
goals in a latent space. This paper provides a solution, which is learned entirely
end-to-end. Moreover, they train the manager network using a transition
policy gradient to output goals, which are semantically meaningful sub-goals.
This paper is an outlier in so far as it is the only work we have
reviewed, which is based on feudal reinforcement learning.

One reason to use hirearchical reinforcement learning is
to decompose the state space and learn a separate value function
for each region of the state space in order to reduce the complexity of the reward
function that any single value function must approximate. The paper
[Hybrid Reward Architecture for Reinforcement Learning] builds off this idea
by decomposing the and reward function and learning seperate value funtions for
each component of the reward function. The main contribution of this papers is
the formulation of a Hybrid Reward Architecture that takes as input a decomposed
reward function trains a seperate heads for each component on top of a shared based
network using the DQN [cite DQN] algorithm. This architecture has the advantage of
removing irrelevant features and is able to learn more quickly using the decomposed
reward function, which is smoother and easier to optimize.




We discuss future and ongoing challanges in the following section.
One issue is that it is not easy to compare these methods because
everyone is using different datasets. Also because certain methods
still require some hand engineering of a proxy reward function for instance
it is not even possible to port the methods to another environment without
making new design decisions.

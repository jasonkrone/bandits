\section{Recent Works on Hierarchical Reinforcement Learning}

To a large degree recent work on hierarchical reinforcement learning focuses on leveraging recent
\footnote{For the purpose of this section we assume that recent works are papers released in 2016 or 2017}
advances in Deep Learning to augment the foundational hierarchical methods, which we described in the previous section.
Consequently, recent papers are less theoretical and focus on techniques to improve the performance
of existing methods when using neural networks as well as the application of hierarchical reinforcement learning
to new, more complex environments, such as the Atari suite of games.
This section discusses the contribution of eight recent works. While undoubtedly this list leaves out a number
of recent hierarchical papers, it is our hope that the works selected provide a balanced view of the recent
research direction. We organize this section by grouping papers with similar contributions, and discussing
these groups roughly in order of most to least theoretical.

Before diving into the contributions made in these recent works, it is worth noting which of the frameworks
(options, HAMs, Feudal RL, MaxQ) are used most often as the foundation for this new research.
Options are by far the most popular base for these recent methods; specifically, seven of the eight
papers that we discuss utilize the options framework. In contrast, only one paper bases their work
on Feudal Reinforcement learning, and no papers extend HAMs or MaxQ. Although the reason for this is
not entirely clear, our view is that the options framework was more consistently developed in the interim
between its conception in 1999 and 2015, which made it more appealing to build upon. Moreover, the options
framework is more intuitive than HAM or MaxQ because an option essentially functions as a sub-policy that is
selected like an action, and it is therefore quite easy to understand options given knowledge of MDPs and policies.

\begin{subsection}{End-to-end Learning of Options}
The most theoretical work is \cite{Bacon}, which extends options to allow for end-to-end learning
of internal policies and termination conditions for a given number of options using
linear or non-linear function approximators. The primary contribution of this work is the proof
of policy gradient theorems for options, which allow for the end-to-end learning of options.
This is a significant development of the options framework because, to the best of our knowledge,
all applications of Deep Learning to options aside from this paper have required either the internal policies
or termination conditions to be pre-trained or pre-defined. While, the end-to-end learning of both internal policies
and termination conditions is not yet prevalent in the literature, it is a logical extension, which gives superior
performance, and will likely gain traction going forward. It is worth noting that one criticism of this work,
, made in \cite{Florensa}, is that the options learned may be overly specific to the task at hand and not transfer well
to new similar tasks.
\end{subsection}

\begin{subsection}{Replacing Hand Engineering with Deep Learning}
Although no other works have learned the internal policies and termination conditions
for the options they use, both \cite{Tejas} and
\cite{Florensa} work to reduce the amount of hand engineering required
to use options. \cite{Florensa} focuses on learning internal policies for options in an unsupervised manner
during a separate pre-training phase using a flexible proxy reward.
Specifically, \cite{Florensa} found that Stochastic neural networks coupled with
a mutual information regularizer and a proxy reward, such as the speed of a
robot in the continuous control setting, can reliably learn internal policies for
options with minimal supervision. In contrast,
\cite{Tejas} jointly learns both the policy over options
as well as the internal policy used when executing the options. Both papers choose to
use a single neural network to approximate all internal option policies. They do this
by conditioning the network on a goal $g$, which can be thought of as selecting the
appropriate internal option for the network to approximate. Using a single network
to approximate all internal option policies has the benefit of sharing knowledge
across similar tasks as well as allowing space complexity to remain constant as
the number of options grow.

Similarly, the model used in \cite{Vezhnevets} learns both sub-manager policies and
goals in a latent space. This paper provides a solution, which is learned entirely
end-to-end. Moreover, they train the manager network using a transition
policy gradient to output goals, which are semantically meaningful sub-goals.
This paper is an outlier in so far as it is the only work we have
reviewed, which is based on feudal reinforcement learning.
\end{subsection}

\begin{subsection}{Reward Function Decomposition}
One reason to use hierarchical reinforcement learning is
to decompose the state space and learn a separate value function
for each region of the state space in order to reduce the complexity of the reward
function that any single value function must approximate. The paper
\cite{Seijen} builds off this idea
by decomposing the reward function and then learning separate value functions for
each component of the reward function. The main contribution of this paper is
the formulation of a Hybrid Reward Architecture that takes as input a decomposed
reward function and trains a separate head for each component on top of a shared base
network using the DQN algorithm \cite{Mnih}. This architecture has the advantage of
removing irrelevant features and is able to learn more quickly using the decomposed
reward function, which is smoother and easier to optimize.
\end{subsection}

\begin{subsection}{Hierarchical Knowledge Distilation}
We have discussed how both \cite{Florensa} and \cite{Tejas}
use a single neural network conditioned on a goal to approximate the policies of each option.
In addition to training such a network from scratch, it is also possible to distil
the knowledge from many expert internal policy networks to a single network with an
output head for each internal policy. This is the approach taken in \cite{Tessler}, which
demonstrates that distilation enables knowledge sharing among tasks and increases performance.
This work extends the DQN algorithm by formulating a novel
experience reply buffer and objective function that are compatible with the use
of options.
\end{subsection}

\begin{subsection}{Applications: Conversational Agents}
An application of particular interest in Hierarchical Reinforcement learning
is conversational agents. This is a natural application of hierarchical
reinforcement learning because the tasks that conversational agents are asked
to complete are often very similar and therefore can share sub-routines
(i.e. options). For instance, a travel agent system will need to book
and pay for a number of items such as a hotel rooms, cars, and plane tickets.
Therefore, it makes sense that booking and payment skills should be shared sub-routines.
Both \cite{Peng} and \cite{Budzianowski} apply hierarchical
reinforcement learning to the task of dialogue management. \cite{Budzianowski}
uses pseudo rewards to train gaussian process policies for booking and payment options,
which are shared by agents that make hotel and restaurant reservations respectively.
Similarly, \cite{Peng} applies the options using neural networks
to dialogue management for the subtasks of booking an airplane ticket and reserving a hotel.
However, \cite{Peng} uses options within a very large model that
includes Natural Language Generation and Natural Language Understanding
submodules, while \cite{Budzianowski} focuses almost solely on options and
uses the PyDial dialogue modeling tool-kit to evaluate their architecture.
\end{subsection}


